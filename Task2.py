# -*- coding: utf-8 -*-
"""Task2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e4D_C3ZBnWpn0GX7Ul4VuPpC1bAGmYYM
"""

import nltk
import re
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt_tab')

from google.colab import files
uploaded = files.upload()

import io
import pandas as pd

df = pd.read_csv(io.BytesIO(uploaded['Data Collection - Task 1 - Sheet1 (1).csv']))

# Remove rows with missing values in essential columns
df = df.dropna(subset=["Comments", "Reviews", "Platform "])

#Clean text data (remove extra spaces, lowercase, etc.)
# Standardize column names first
df.columns = df.columns.str.strip()

df["Comments"] = df["Comments"].str.strip().str.lower()
df["Reviews"] = df["Reviews"].str.strip().str.lower()
df["Platform"] = df["Platform"].str.strip().str.lower() # Accessing 'Platform' without the trailing space
print(df.head())
print(df.tail())

#Remove duplicates
df = df.drop_duplicates()

#Reset index after cleaning
df = df.reset_index(drop=True)
print(df.head())
print(df.tail())

#Final cleaned dataset
print(df.head())
print(df.tail())
print("\nShape after preprocessing:", df.shape)

#Standardize column names (remove spaces/newlines)
df.columns = df.columns.str.strip()

#Simple stopwords list (offline, no downloads)
stop_words = set([
    "the","a","an","is","it","this","that","and","or","in","on","for","of","to","with",
    "was","were","be","been","am","are","as","at","by","from","but","so","if","then",
    "too","very","just","can","will","do","does","did"
])
print(df.head())
print(df.tail())

import string

# Preprocessing function
def preprocess_text(text):
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Tokenize and remove stopwords
    tokens = [word for word in text.split() if word not in stop_words]
    return " ".join(tokens)
    # Apply stemming preprocessing
df["Stemmed_Comments"] = df["Comments"].apply(preprocess_text)

print(df.head())
print(df.tail())

# Function to remove emojis
def remove_emojis(text):
    text = str(text)
    emoji_pattern = re.compile(
        "["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags
        u"\U00002700-\U000027BF"  # dingbats
        u"\U0001F900-\U0001F9FF"  # supplemental symbols
        u"\U00002600-\U000026FF"  # miscellaneous symbols
        "]+",
        flags=re.UNICODE
    )
    return emoji_pattern.sub(r'', text)

# Example usage
df["No_Emoji_Comments"] = df["Comments"].apply(remove_emojis)

print(df.head())

# --- Custom Word-level Tokenizer ---
def word_tokenize_custom(text):
    text = str(text).lower()
    # Split by words (alphanumeric only)
    tokens = re.findall(r'\b\w+\b', text)
    return tokens

# --- Custom Sentence-level Tokenizer ---
def sentence_tokenize_custom(text):
    text = str(text)
    # Split by punctuation marks (., !, ?)
    sentences = re.split(r'(?<=[.!?]) +', text)
    return [s.strip() for s in sentences if s]

# Apply on dataset
df["Word_Tokens"] = df["Stemmed_Comments"].apply(word_tokenize_custom)
df["Sentence_Tokens"] = df["Comments"].apply(sentence_tokenize_custom)

# Show result
display(df.head(100))